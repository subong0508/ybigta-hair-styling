{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pix2Pix\n",
    "\n",
    "#### YBIGTA 18기 남종현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic\n",
    "\n",
    "+ 대부분의 CNN방식에서 유클리디안 거리를 최소화 하는 쪽으로 진행\n",
    "+ 나올 수 있는 결과의 평균을 취하는 쪽으로 진행하기 때문에 이미지가 선명하지 못함\n",
    "+ pix2pix2는 GAN을 기반으로 진행하기 때문에 이러한 결과 방지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](imgs/cGAN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ cGAN 기반\n",
    "+ Input vector x와 noise vector z를 이용해 output 생성\n",
    "+ Input image의 dist 정보도 real/fake를 구분하는 데 사용\n",
    "+ dropout의 형식으로 noise 제공"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](imgs/cGANLoss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](imgs/L1Loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cGAN Loss(adversarial Loss) + L1 Loss(reconstruction Loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ D는 real/fake를 구분하는 역할을 그대로 가짐\n",
    "+ G는 D를 속이는 것 이외에 ground truth에 가까운 이미지를 생성해야함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### final objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](imgs/pix2pixLoss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ DCGAN의 G와 D를 기본 모델로 했고, 각 layer는 conv-BatchNorm-ReLU 구조를 따름"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](imgs/U-Net.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 이전의 연구들은 encoder-decoder Network를 사용\n",
    "+ bottleneck를 통과하며 정보의 손실이 발생\n",
    "+ 따라서 skip-connection을 추가한 U-Net 구조 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### U-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](imgs/U-Net1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 전체 layer 수를 N이라고 할때, i번째와 N-i번째 layer를 연결\n",
    "+ 각 연결은 단순히 concatenate\n",
    "+ 처음 detail들이 마지막 layer까지 잘 전달 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Ck: conv-BatchNorm-ReLU layer\n",
    "+ CDk: conv-BatchNorm-Dropout-ReLU layer with a dropout rate of 50%\n",
    "+ All conv는 4x4 spatial filter, stride = 2\n",
    "+ downsample, upsample by a factor 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generator layers\n",
    "\n",
    "Encoder: C64-C128-C256-C512-C512-C512-C512-C512\n",
    "\n",
    "U-Net decoder: CD512-CD1024-CD1024-C1024-C1024-C512-C256-C128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ decoder의 마지막 layer 이후 Tanh 사용\n",
    "+ encoder의 C64에서는 BatchNorm 적용 x\n",
    "+ encoder의 ReLu는 leaky ReLU(slope=0.2), decoder는 ReLU 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](imgs/PatchGAN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ PatchGAN 사용\n",
    "+ NxN patch를 사용하여 각 부분의 real/fake 판별\n",
    "+ 전체 이미지를 연산하여 판별하는 것 보다 연산의 수가 적고 빠름\n",
    "+ G는 각각의 이미지 patch 조각의 real/fake를 속이기 위해 학습과정 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator layers\n",
    "\n",
    "70x70 discriminator\n",
    "\n",
    "C64-C128-C256-C512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 마지막 layer 이후 1-dim output return(sigmoid func)\n",
    "+ 첫번째 layer에서 BatchNorm 적용 x\n",
    "+ Leaky ReLU(slope = 0.2) 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Train\n",
    "\n",
    "Jittering: 256x256 이미지는 286x286 크기로 resize, random cropping을 통해\n",
    "           256x256으로 변환\n",
    "\n",
    "모든 네트워크는 scratch로부터 학습\n",
    "\n",
    "weight는 Gaussian dist(0, 0.02)를 따르는 랜덤 초기값을 가짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Optimization\n",
    "\n",
    "일반적인 GAN 접근법을 따름\n",
    "\n",
    "D를 최적화하는 Objective를 2로 나누어 D가 G보다 상대적으로 빠르게 학습되지 않도록 함\n",
    "\n",
    "minibatch SGD, Adam(lr = 0.0002, beta = (0.5, 0.999) ) 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "\n",
    "https://arxiv.org/abs/1611.07004\n",
    "\n",
    "https://greeksharifa.github.io/generative%20model/2019/04/07/Pix2Pix/\n",
    "\n",
    "https://medium.com/humanscape-tech/paper-review-pix2pix-20418569e0c1\n",
    "\n",
    "https://github.com/phillipi/pix2pix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
